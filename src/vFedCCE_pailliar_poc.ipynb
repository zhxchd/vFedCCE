{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vFedCCE_pailliar_PoC.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uN7CVyRyvmOZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrS5l6QhiB-f"
      },
      "source": [
        "# Federated Deep Learning on Vertically Partitioned SGCP Dataset - Proof of Concept on Pailliar Encryption Scheme\n",
        "\n",
        "By Xiaochen Zhu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krJ5flk0hXFt"
      },
      "source": [
        "## Background\n",
        "\n",
        "This notebook is an implementation of `vFedCCE` which is a private deep learning method using categorical cross entropy loss and gradient optimization to solve multi-category classfication problem in vertically partitioned datasets where labelled are only stored in one of the clients.\n",
        "\n",
        "This particular implementation is an example when there are two categories but the same method applies to all classfication problem where categorical cross entropy loss function is the minimization goal.\n",
        "\n",
        "This notebook aims to demonstrate the use of Pailliar encryption scheme, a widely used additively homomorphic encryption technique. Due to the computational complexity of encryption, this is a proof of concept of the technique instead of a full implementation of the original algorithm with encryption. To reduce the computational complexity, only one batch will be ran in this notebook to demonstrate the encryption feasibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvwbDDFvz_4q"
      },
      "source": [
        "## Set up environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYeLUKxWuHcp"
      },
      "source": [
        "If you encounter error when running these `import`s, please restart runtime and try again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1detcQtmh-Ez"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "import seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers.experimental import preprocessing\n",
        "import math\n",
        "import uuid\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "! pip -q install phe\n",
        "from phe import paillier\n",
        "\n",
        "# ! pip -q install clkhash\n",
        "# from clkhash import clk, randomnames"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO0xTwUpj37u"
      },
      "source": [
        "## Data preperation\n",
        "\n",
        "We need to load the data and vertically partition the dataset into two clients where each of them will have half the features and one of them will store the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwhIo52XwTdz"
      },
      "source": [
        "### Load and vertically partition the data\n",
        "\n",
        "Just load the complete `csv` file into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOCQJ1kBVwUF",
        "outputId": "40d44e25-5baa-4fa4-e6f1-66e30743ac31"
      },
      "source": [
        "# Download the zip file from the internet and unzip it\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00573/SouthGermanCredit.zip\n",
        "with zipfile.ZipFile('SouthGermanCredit.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./SouthGermanCredit/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-06 19:51:43--  https://archive.ics.uci.edu/ml/machine-learning-databases/00573/SouthGermanCredit.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13130 (13K) [application/x-httpd-php]\n",
            "Saving to: ‘SouthGermanCredit.zip.2’\n",
            "\n",
            "SouthGermanCredit.z 100%[===================>]  12.82K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-07-06 19:51:44 (90.1 MB/s) - ‘SouthGermanCredit.zip.2’ saved [13130/13130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Mt62_qtCj9zC",
        "outputId": "b75b30d1-35de-4f41-9d8a-7425f86eec92"
      },
      "source": [
        "original_df = pd.read_csv('./SouthGermanCredit/SouthGermanCredit.asc', sep=' ')\n",
        "original_df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>laufkont</th>\n",
              "      <th>laufzeit</th>\n",
              "      <th>moral</th>\n",
              "      <th>verw</th>\n",
              "      <th>hoehe</th>\n",
              "      <th>sparkont</th>\n",
              "      <th>beszeit</th>\n",
              "      <th>rate</th>\n",
              "      <th>famges</th>\n",
              "      <th>buerge</th>\n",
              "      <th>wohnzeit</th>\n",
              "      <th>verm</th>\n",
              "      <th>alter</th>\n",
              "      <th>weitkred</th>\n",
              "      <th>wohn</th>\n",
              "      <th>bishkred</th>\n",
              "      <th>beruf</th>\n",
              "      <th>pers</th>\n",
              "      <th>telef</th>\n",
              "      <th>gastarb</th>\n",
              "      <th>kredit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "      <td>1000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.577000</td>\n",
              "      <td>20.903000</td>\n",
              "      <td>2.54500</td>\n",
              "      <td>2.828000</td>\n",
              "      <td>3271.24800</td>\n",
              "      <td>2.105000</td>\n",
              "      <td>3.384000</td>\n",
              "      <td>2.973000</td>\n",
              "      <td>2.68200</td>\n",
              "      <td>1.145000</td>\n",
              "      <td>2.845000</td>\n",
              "      <td>2.358000</td>\n",
              "      <td>35.54200</td>\n",
              "      <td>2.675000</td>\n",
              "      <td>1.928000</td>\n",
              "      <td>1.407000</td>\n",
              "      <td>2.904000</td>\n",
              "      <td>1.845000</td>\n",
              "      <td>1.404000</td>\n",
              "      <td>1.963000</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.257638</td>\n",
              "      <td>12.058814</td>\n",
              "      <td>1.08312</td>\n",
              "      <td>2.744439</td>\n",
              "      <td>2822.75176</td>\n",
              "      <td>1.580023</td>\n",
              "      <td>1.208306</td>\n",
              "      <td>1.118715</td>\n",
              "      <td>0.70808</td>\n",
              "      <td>0.477706</td>\n",
              "      <td>1.103718</td>\n",
              "      <td>1.050209</td>\n",
              "      <td>11.35267</td>\n",
              "      <td>0.705601</td>\n",
              "      <td>0.530186</td>\n",
              "      <td>0.577654</td>\n",
              "      <td>0.653614</td>\n",
              "      <td>0.362086</td>\n",
              "      <td>0.490943</td>\n",
              "      <td>0.188856</td>\n",
              "      <td>0.458487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>250.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>19.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1365.50000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>27.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>2.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2319.50000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>33.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3972.25000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>42.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>18424.00000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>75.00000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          laufkont     laufzeit  ...      gastarb       kredit\n",
              "count  1000.000000  1000.000000  ...  1000.000000  1000.000000\n",
              "mean      2.577000    20.903000  ...     1.963000     0.700000\n",
              "std       1.257638    12.058814  ...     0.188856     0.458487\n",
              "min       1.000000     4.000000  ...     1.000000     0.000000\n",
              "25%       1.000000    12.000000  ...     2.000000     0.000000\n",
              "50%       2.000000    18.000000  ...     2.000000     1.000000\n",
              "75%       4.000000    24.000000  ...     2.000000     1.000000\n",
              "max       4.000000    72.000000  ...     2.000000     1.000000\n",
              "\n",
              "[8 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKErxmSqESKH"
      },
      "source": [
        "Give all entries a uuid and shuffle the two datasets so that later they can find matches based on that uuid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELhWgppS89-a"
      },
      "source": [
        "id = pd.Series(range(0,1000)).apply(lambda i : str(uuid.uuid4()))\n",
        "df_with_id = original_df.copy()\n",
        "df_with_id['id'] = id\n",
        "df_with_id = df_with_id.set_index('id')\n",
        "client1_data = df_with_id[['moral','verw','beszeit','famges','wohnzeit','alter','wohn','beruf','telef','gastarb']]#.sample(frac=1)\n",
        "client2_data = df_with_id[['laufkont','laufzeit','hoehe','sparkont','rate','buerge','verm','weitkred','bishkred','pers','kredit']]#.sample(frac=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB3w2PrHC-2S",
        "outputId": "9796cd3c-5bb2-4bb1-e3c1-5604deeeaa44"
      },
      "source": [
        "# Client 1 has 10 features, no labels and the entries are shuffled\n",
        "# client 2 has 11 features and labels and the entries are shuffled\n",
        "print(client1_data)\n",
        "print(client2_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                      moral  verw  ...  telef  gastarb\n",
            "id                                                 ...                \n",
            "677a5bbf-0ef5-48f7-b960-9fb9ea564cf9      4     2  ...      1        2\n",
            "ee6deefa-8b94-434b-9918-d949b763d602      4     0  ...      1        2\n",
            "c177b067-accc-4b42-9672-25c4c7f86cba      2     9  ...      1        2\n",
            "f6b8cef6-16d5-43d2-8588-0222959754d3      4     0  ...      1        1\n",
            "787aaa76-64ae-4d25-a4f1-44d7a169ce40      4     0  ...      1        1\n",
            "...                                     ...   ...  ...    ...      ...\n",
            "0b12cfea-acef-4e6d-8dde-f3c338414152      2     3  ...      1        2\n",
            "9be219fa-2909-4efe-8b7d-76de34e92b11      2     0  ...      1        2\n",
            "ca6658a8-4de4-4489-a475-03abc7b2f46c      4     0  ...      2        2\n",
            "eac08885-26ee-4442-8616-4fbbaffcb8e2      2     3  ...      2        2\n",
            "5c8c5733-33cb-414b-96be-c845da911f06      2     2  ...      1        2\n",
            "\n",
            "[1000 rows x 10 columns]\n",
            "                                      laufkont  laufzeit  ...  pers  kredit\n",
            "id                                                        ...              \n",
            "677a5bbf-0ef5-48f7-b960-9fb9ea564cf9         1        18  ...     2       1\n",
            "ee6deefa-8b94-434b-9918-d949b763d602         1         9  ...     1       1\n",
            "c177b067-accc-4b42-9672-25c4c7f86cba         2        12  ...     2       1\n",
            "f6b8cef6-16d5-43d2-8588-0222959754d3         1        12  ...     1       1\n",
            "787aaa76-64ae-4d25-a4f1-44d7a169ce40         1        12  ...     2       1\n",
            "...                                        ...       ...  ...   ...     ...\n",
            "0b12cfea-acef-4e6d-8dde-f3c338414152         1        24  ...     1       0\n",
            "9be219fa-2909-4efe-8b7d-76de34e92b11         1        24  ...     2       0\n",
            "ca6658a8-4de4-4489-a475-03abc7b2f46c         4        21  ...     2       0\n",
            "eac08885-26ee-4442-8616-4fbbaffcb8e2         2        12  ...     2       0\n",
            "5c8c5733-33cb-414b-96be-c845da911f06         1        30  ...     2       0\n",
            "\n",
            "[1000 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tas5fy5MpGUU"
      },
      "source": [
        "### Train/test split (overlapping)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFexwfz2pS97"
      },
      "source": [
        "client1_train, client1_test = train_test_split(client1_data, test_size=0.2, random_state=69)\n",
        "client2_train = client2_data.loc[client1_train.index]\n",
        "client2_test = client2_data.loc[client1_test.index]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrw-mnCyWtF5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olCDKs_wUA0d"
      },
      "source": [
        "### Train/test datasets info "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4scfoxvT89a",
        "outputId": "58877f2e-4272-4562-f99a-b669f2652131"
      },
      "source": [
        "common_train_index = client1_train.index.intersection(client2_train.index)\n",
        "common_test_index = client1_test.index.intersection(client2_test.index)\n",
        "\n",
        "print(\n",
        "    'There are {} common entries (out of {}) in client 1 and client 2\\'s training datasets,\\nand {} common entries (out of {}) in their test datasets'\n",
        "    .format(\n",
        "        len(common_train_index),\n",
        "        len(client1_train),\n",
        "        len(common_test_index),\n",
        "        len(client1_test)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 800 common entries (out of 800) in client 1 and client 2's training datasets,\n",
            "and 200 common entries (out of 200) in their test datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-4a_1wPuYCm"
      },
      "source": [
        "## `vFedCCE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLux0WFF5TSg"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL-9Jwyh2SK8"
      },
      "source": [
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "epochs = 1\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# Instantiate a loss function.\n",
        "# Not from logits because of the softmax layer converting logits to probability.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "# Instantiate a metric function (accuracy)\n",
        "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7siRHZKif-Ry"
      },
      "source": [
        "### The `Client` class\n",
        "\n",
        "Client 1 and client 2 both have half the features. Client 2 also stores the labels. If before each update of parameters, client 1 can send its partial prediction and other intermediate data to client 2, client 2 will be able to calculate the total loss and update the total model and give that updated model back to client 1.\n",
        "\n",
        "Note that in this iteration of implementation, I do not plan to add implementation of entity resolution. Therefore, the examples with the same index on the two clients is ensured to point to the same end user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvxl9mxjgJqg"
      },
      "source": [
        "class Client:\n",
        "\n",
        "  def __init__(self, train_data, test_data, labelled):\n",
        "    self.__trainX = train_data.copy()\n",
        "    self.__testX = test_data.copy()\n",
        "    self.labelled = labelled\n",
        "\n",
        "    if (labelled):\n",
        "      self.__trainY = self.__trainX.pop('kredit')\n",
        "      self.__testY = self.__testX.pop('kredit')\n",
        "    else:\n",
        "      self.public_key, self.private_key = paillier.generate_paillier_keypair(n_length=256)\n",
        "\n",
        "    normalizer = preprocessing.Normalization()\n",
        "    normalizer.adapt(np.array(self.__trainX.loc[common_train_index]))\n",
        "\n",
        "    self.model = tf.keras.Sequential([\n",
        "      normalizer,\n",
        "      layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(128, activation='elu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "      layers.Dropout(0.5),\n",
        "      layers.Dense(2),\n",
        "      layers.Softmax()])\n",
        "    \n",
        "    self.shapes = [[10,128], [128], [128,128], [128], [128,2], [2]]\n",
        "    \n",
        "  def next_batch(self, index):\n",
        "    self.batchX = self.__trainX.loc[index]\n",
        "    if not self.labelled:\n",
        "      grads = []\n",
        "      self.model_output = np.zeros((len(index), 2))\n",
        "      for i in range(len(index)):\n",
        "        with tf.GradientTape() as gt:\n",
        "          gt.watch(self.model.trainable_weights)\n",
        "          output_by_example = self.model(self.batchX.iloc[i:i+1], training=True)\n",
        "          output_for_grad = output_by_example[:,1]\n",
        "        self.model_output[i] = output_by_example\n",
        "        grads.append(gt.gradient(output_for_grad, self.model.trainable_weights))\n",
        "      return grads, [[[self.public_key.encrypt(i) for i in tf.reshape(x, [-1]).numpy().tolist()] for x in g] for g in grads]\n",
        "    else:\n",
        "      self.batchY = self.__trainY.loc[index]\n",
        "      with tf.GradientTape() as self.gt:\n",
        "        self.gt.watch(self.model.trainable_weights)\n",
        "        self.model_output = self.model(self.batchX, training=True)\n",
        "\n",
        "  def cal_model(self):\n",
        "    return self.model_output\n",
        "  \n",
        "  def predict(self, test_index):\n",
        "    return self.model.predict(self.__testX.loc[test_index])# + 1e-8\n",
        "\n",
        "  def test_answers(self, test_index):\n",
        "    if self.labelled:\n",
        "      return self.__testY.loc[test_index]\n",
        "  \n",
        "  def batch_answers(self):\n",
        "    if self.labelled:\n",
        "      return self.batchY\n",
        "\n",
        "  def loss_and_update(self, a):\n",
        "    if not self.labelled:\n",
        "      raise AssertionError(\"This method can only be called by client 2\")\n",
        "    self.prob = (a + self.model_output)/2\n",
        "    self.c = self.coefficient_and_update()/len(self.batchX)\n",
        "    return self.prob, loss_fn(self.batchY, self.prob)\n",
        "  \n",
        "  def coefficient_and_update(self):\n",
        "    if not self.labelled:\n",
        "      raise AssertionError(\"This method can only be called by client 2\")\n",
        "    p = self.prob[:,1]\n",
        "    c = (p-self.batchY)/((p)*(1-p))\n",
        "    with self.gt:\n",
        "      output = sum(c * self.model_output[:,1])/len(c)\n",
        "    grads = self.gt.gradient(output, self.model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "    return c\n",
        "  \n",
        "  def update_with_plain(self, grads):\n",
        "    weights = self.model.trainable_weights\n",
        "    optimizer.apply_gradients(zip(grads, weights))\n",
        "    return weights\n",
        "\n",
        "  def update_with_cipher(self, cipher_grads):\n",
        "    weights = self.model.trainable_weights\n",
        "    # cipher_grads is a list of six lists\n",
        "    for i in range(len(cipher_grads)):\n",
        "      cipher_grads[i] = [self.private_key.decrypt(x) for x in cipher_grads[i]]\n",
        "      cipher_grads[i] = tf.reshape(tf.convert_to_tensor(cipher_grads[i]), self.shapes[i])\n",
        "    optimizer.apply_gradients(zip(cipher_grads, weights))\n",
        "    return weights\n",
        "\n",
        "  def assemble_grad_plain(self, partial_grads):\n",
        "    if not self.labelled:\n",
        "      raise AssertionError(\"This method can only be called by client 2\")\n",
        "    # to assemble the gradient for client 1\n",
        "    for i in range(len(self.c)):\n",
        "      partial_grads[i] = [x * self.c[i] for x in partial_grads[i]]\n",
        "    return [sum(x) for x in zip(*partial_grads)]\n",
        "  \n",
        "  def assemble_grad_cipher(self, cipher_partial_grads):\n",
        "    if not self.labelled:\n",
        "      raise AssertionError(\"This method can only be called by client 2\")\n",
        "    # assemble the cipher gradient for client 1 from cipher partial gradients\n",
        "    for i in range(len(self.c)):\n",
        "      # cipher_partial_grads[i] is a list of 6 lists, c[i] is a float\n",
        "      # x in cipher_partial_grads[i] is a 1d list flatten from gradients\n",
        "      cipher_partial_grads[i] = [[t * float(self.c[i]) for t in x] for x in cipher_partial_grads[i]]\n",
        "    return [[sum(i) for i in zip(*x)] for x in zip(*cipher_partial_grads)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDy-LsCOgNWp"
      },
      "source": [
        "client1 = Client(client1_train, client1_test, False)\n",
        "client2 = Client(client2_train, client2_test, True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNpoYAT_5a4j"
      },
      "source": [
        "### Trial run on single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SwV1kvvr5vg"
      },
      "source": [
        "# train_index_batches = [common_train_index[i:i + batch_size] for i in range(0, len(common_train_index), batch_size)] \n",
        "common_train_index_list = common_train_index.to_list()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcQA4Az_ucKL"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  random.shuffle(common_train_index_list)\n",
        "  train_index_batches = [common_train_index_list[i:i + batch_size] for i in range(0, len(common_train_index_list), batch_size)]\n",
        "  total_loss = 0.0\n",
        "  # Only iterate over the first batch to prove the concept.\n",
        "  train_index_batches = train_index_batches[:1]\n",
        "  for step, batch_index in enumerate(train_index_batches):\n",
        "    \n",
        "    plain_partial_grads, cipher_partial_grads = client1.next_batch(batch_index)\n",
        "    client2.next_batch(batch_index)\n",
        "\n",
        "    prob, loss_value = client2.loss_and_update(client1.cal_model())\n",
        "    cipher_grad = client2.assemble_grad_cipher(cipher_partial_grads)\n",
        "    plain_grad = client2.assemble_grad_plain(plain_partial_grads)\n",
        "    weights_from_cipher = client1.update_with_cipher(cipher_grad)\n",
        "    weights_from_plain = client1.update_with_plain(plain_grad)\n",
        "    \n",
        "    # No need to record train loss or accuracy, or further predictions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eirf9qyvV4sU"
      },
      "source": [
        "### Compare the two gradients\n",
        "\n",
        "Then we can show that the two weights (updated with encrypted gradient or updated with plain gradient) are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaoQCq9Qyyc",
        "outputId": "67b8a2fa-c2dc-48c8-bad0-5cb5731be9dd"
      },
      "source": [
        "diff = [weights_from_cipher[i] - weights_from_plain[i] for i in range(len(weights_from_cipher))]\n",
        "print(diff)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(10, 128), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
            "array([[0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
            "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Tensor: shape=(128, 2), dtype=float32, numpy=\n",
            "array([[0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.],\n",
            "       [0., 0.]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}